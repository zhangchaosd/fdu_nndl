
'''
输入为一个维度是 N * C * H * W 的张量 X，经过一个卷积核大小为3，padding为1的卷积层和一个全连接层，最后经过softmax

Input a tensor X with dimension N * C * H * W, through a convolution kernel of size 3, padding 1 convolution layer and a full-connection layer, and finally through Softmax.
x = torch.tensor([[[[0.9333, 0.2035, 0.9389, 0.9961, 0.9181, 0.9640, 0.5843, 0.3964,
           0.4917],
          [0.7420, 0.7840, 0.8005, 0.4626, 0.6545, 0.6895, 0.6448, 0.6957,
           0.0884],
          [0.4627, 0.1267, 0.6657, 0.6428, 0.7052, 0.3818, 0.9360, 0.3617,
           0.1984],
          [0.0334, 0.8370, 0.9924, 0.1335, 0.5554, 0.3673, 0.2153, 0.4351,
           0.8061],
          [0.0381, 0.4261, 0.4165, 0.2291, 0.3426, 0.9223, 0.9077, 0.0359,
           0.0808],
          [0.5045, 0.6250, 0.5848, 0.7800, 0.4321, 0.5936, 0.9047, 0.5769,
           0.7873],
          [0.9817, 0.8802, 0.7662, 0.2750, 0.9204, 0.2961, 0.7736, 0.2086,
           0.0850],
          [0.4546, 0.3215, 0.6589, 0.0927, 0.9310, 0.2547, 0.6322, 0.1983,
           0.6418],
          [0.1040, 0.3974, 0.6637, 0.4836, 0.0409, 0.7771, 0.8754, 0.5349,
           0.6465]],

         [[0.4738, 0.3065, 0.5351, 0.1389, 0.1792, 0.2233, 0.5594, 0.6454,
           0.7789],
          [0.8185, 0.1344, 0.5356, 0.8648, 0.6502, 0.8910, 0.1248, 0.8504,
           0.6650],
          [0.3893, 0.0117, 0.7504, 0.7489, 0.7495, 0.4241, 0.5850, 0.3442,
           0.4146],
          [0.9498, 0.9598, 0.8278, 0.4415, 0.1627, 0.5080, 0.4001, 0.4953,
           0.7812],
          [0.2046, 0.4291, 0.3384, 0.3138, 0.6421, 0.2448, 0.8619, 0.1459,
           0.9772],
          [0.9479, 0.9009, 0.3968, 0.4434, 0.2744, 0.6949, 0.3678, 0.4621,
           0.0851],
          [0.0395, 0.4376, 0.9183, 0.1712, 0.2280, 0.3206, 0.0265, 0.6734,
           0.3158],
          [0.1646, 0.2709, 0.0842, 0.8044, 0.1827, 0.0678, 0.7215, 0.8764,
           0.8994],
          [0.3636, 0.4405, 0.4860, 0.5276, 0.7164, 0.8434, 0.2736, 0.8453,
           0.3439]],

         [[0.7897, 0.2918, 0.3805, 0.1987, 0.1244, 0.8104, 0.6746, 0.2396,
           0.1905],
          [0.3614, 0.4804, 0.8724, 0.5455, 0.5926, 0.7103, 0.7613, 0.9540,
           0.9259],
          [0.1181, 0.7905, 0.1686, 0.8942, 0.2065, 0.1390, 0.3689, 0.7606,
           0.8941],
          [0.2737, 0.9051, 0.0305, 0.1286, 0.9150, 0.2689, 0.4834, 0.9412,
           0.0413],
          [0.0903, 0.6222, 0.9121, 0.6492, 0.9043, 0.6111, 0.8854, 0.4364,
           0.1329],
          [0.3483, 0.7235, 0.7983, 0.2623, 0.5393, 0.2006, 0.8146, 0.7095,
           0.2098],
          [0.3539, 0.7776, 0.5593, 0.4980, 0.8771, 0.9352, 0.9983, 0.3237,
           0.2850],
          [0.4139, 0.8331, 0.2679, 0.2875, 0.2279, 0.7491, 0.3661, 0.3221,
           0.5194],
          [0.7049, 0.8221, 0.3258, 0.8557, 0.4698, 0.5593, 0.7746, 0.1148,
           0.4297]]]])
ret = torch.tensor([[0.0941, 0.0816, 0.1132, 0.0985, 0.1197, 0.0769, 0.1123, 0.0967, 0.1023, 0.1048]])
'''

import torch
import torch.nn as nn
from torch.nn.modules.activation import Softmax

class ConvModel1(nn.Module):
    def __init__(self):
        super(ConvModel1, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels = 3, out_channels = 1, kernel_size=3, padding=1),
        )
        self.fc = nn.Sequential(
            nn.Linear(9 * 9, 10),
            nn.Softmax(dim = -1)
        )

    def forward(self, x):
        out1 = self.conv(x)
        ret = self.fc(out1.reshape(out1.shape[0],-1))
        return ret


'''
输入为一个维度是 N * 3 * 9 * 9 的张量 X，经过一个包含一个卷积核，卷积核大小为3，padding为1的卷积层，得到out1;
将out1在通道维度拼接到x上，再次经过一个包含一个卷积核，卷积核大小为3，padding为1的卷积层和RELU层，得到out2；
将out2经过全连接层和Softmax得到ret。
Input is a tensor X with dimension N * 3 * 9 * 9. After passing through a convolution layer containing a convolution kernel with size 3 and padding 1, out1 is obtained.
After splicing out1 onto X in the channel dimension, out2 is obtained through a convolution layer and RELU layer containing a convolution kernel with a size of 3 and padding of 1.
Out2 is passed through the full connection layer and Softmax to get RET.
'''

'''
x = torch.tensor([[[[0.9333, 0.2035, 0.9389, 0.9961, 0.9181, 0.9640, 0.5843, 0.3964,
           0.4917],
          [0.7420, 0.7840, 0.8005, 0.4626, 0.6545, 0.6895, 0.6448, 0.6957,
           0.0884],
          [0.4627, 0.1267, 0.6657, 0.6428, 0.7052, 0.3818, 0.9360, 0.3617,
           0.1984],
          [0.0334, 0.8370, 0.9924, 0.1335, 0.5554, 0.3673, 0.2153, 0.4351,
           0.8061],
          [0.0381, 0.4261, 0.4165, 0.2291, 0.3426, 0.9223, 0.9077, 0.0359,
           0.0808],
          [0.5045, 0.6250, 0.5848, 0.7800, 0.4321, 0.5936, 0.9047, 0.5769,
           0.7873],
          [0.9817, 0.8802, 0.7662, 0.2750, 0.9204, 0.2961, 0.7736, 0.2086,
           0.0850],
          [0.4546, 0.3215, 0.6589, 0.0927, 0.9310, 0.2547, 0.6322, 0.1983,
           0.6418],
          [0.1040, 0.3974, 0.6637, 0.4836, 0.0409, 0.7771, 0.8754, 0.5349,
           0.6465]],

         [[0.4738, 0.3065, 0.5351, 0.1389, 0.1792, 0.2233, 0.5594, 0.6454,
           0.7789],
          [0.8185, 0.1344, 0.5356, 0.8648, 0.6502, 0.8910, 0.1248, 0.8504,
           0.6650],
          [0.3893, 0.0117, 0.7504, 0.7489, 0.7495, 0.4241, 0.5850, 0.3442,
           0.4146],
          [0.9498, 0.9598, 0.8278, 0.4415, 0.1627, 0.5080, 0.4001, 0.4953,
           0.7812],
          [0.2046, 0.4291, 0.3384, 0.3138, 0.6421, 0.2448, 0.8619, 0.1459,
           0.9772],
          [0.9479, 0.9009, 0.3968, 0.4434, 0.2744, 0.6949, 0.3678, 0.4621,
           0.0851],
          [0.0395, 0.4376, 0.9183, 0.1712, 0.2280, 0.3206, 0.0265, 0.6734,
           0.3158],
          [0.1646, 0.2709, 0.0842, 0.8044, 0.1827, 0.0678, 0.7215, 0.8764,
           0.8994],
          [0.3636, 0.4405, 0.4860, 0.5276, 0.7164, 0.8434, 0.2736, 0.8453,
           0.3439]],

         [[0.7897, 0.2918, 0.3805, 0.1987, 0.1244, 0.8104, 0.6746, 0.2396,
           0.1905],
          [0.3614, 0.4804, 0.8724, 0.5455, 0.5926, 0.7103, 0.7613, 0.9540,
           0.9259],
          [0.1181, 0.7905, 0.1686, 0.8942, 0.2065, 0.1390, 0.3689, 0.7606,
           0.8941],
          [0.2737, 0.9051, 0.0305, 0.1286, 0.9150, 0.2689, 0.4834, 0.9412,
           0.0413],
          [0.0903, 0.6222, 0.9121, 0.6492, 0.9043, 0.6111, 0.8854, 0.4364,
           0.1329],
          [0.3483, 0.7235, 0.7983, 0.2623, 0.5393, 0.2006, 0.8146, 0.7095,
           0.2098],
          [0.3539, 0.7776, 0.5593, 0.4980, 0.8771, 0.9352, 0.9983, 0.3237,
           0.2850],
          [0.4139, 0.8331, 0.2679, 0.2875, 0.2279, 0.7491, 0.3661, 0.3221,
           0.5194],
          [0.7049, 0.8221, 0.3258, 0.8557, 0.4698, 0.5593, 0.7746, 0.1148,
           0.4297]]]])
ret = torch.tensor([[0.1051, 0.1094, 0.1069, 0.1011, 0.1020, 0.0940, 0.0956, 0.0945, 0.0937, 0.0975]])'''
class ConvModel2(nn.Module):
    def __init__(self):
        super(ConvModel2, self).__init__()
        self.conv_relu1 = nn.Sequential(
            nn.Conv2d(in_channels = 3, out_channels = 1, kernel_size=3, padding = 1),
            nn.ReLU(),
        )
        self.conv_relu2 = nn.Sequential(
            nn.Conv2d(in_channels = 4, out_channels = 1, kernel_size=3, padding = 1),
            nn.ReLU()
        )
        self.fc = nn.Sequential(
            nn.Linear(9 * 9, 10),
            nn.Softmax(dim = -1)
        )

    def forward(self, x):
        out1 = self.conv_relu1(x)
        x = torch.cat((x, out1),dim=1)
        out2 = self.conv_relu2(x)
        ret = self.fc(out2.reshape(out1.shape[0],-1))
        return ret

'''
X = torch.tensor([[1,2,3],[4,5,6]])
Y = torch.tensor([[7,8,9],[7,5,3]])
ret = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [7, 5, 3]])
'''
def zy1(X, Y):
    return torch.cat((X, Y), dim=0)


'''
x = torch.tensor([[1,-2,3],[-4,5,-6],[7,-8,9]])
ret = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
'''
import torch
def zy2(x):
    return torch.abs(x)

'''
x = torch.tensor([[1,-2,3],[-4,5,-6],[7,-8,9]])
ret = (torch.tensor([3, 5, 9]), torch.tensor([2, 1, 2]))
'''
import torch
def zy3(x):
    return torch.max(x, dim = 1).values

if __name__ == '__main__':
    X = torch.tensor([[1,-2,3],[-4,5,-6],[7,-8,9]])
    Y = torch.tensor([[7,8,9],[7,5,3]])
    ret = zy3(X)
    print(ret)

